% ISS presentation template
%
% Change history:
% 24.06.2010    Jürgen Ruoff        Initial creation
% 01.07.2010    Patrick Häcker      Generalization
% 02.07.2010    Patrick Häcker      Adjustment
% 15.11.2010    Patrick Häcker      Improvements
% 20.05.2011    Patrick Häcker      Add presentation type
% 06.01.2012	P. Hermannstädter 	Adapted to ISS, small mods
% \graphicspath{ {../Fig/} }
% Insert your name here
\newcommand{\presenter}{Zhuowei Han}
\newcommand{\presentershort}{Z.Han}
\newcommand{\presenteremail}{} 		% can be accessed using \presenteremail
\newcommand{\x}{\mathbf{x}}
\newcommand{\mX}{\mathbf{X}}
\newcommand{\mH}{\mathbf{H}}
\newcommand{\h}{\mathbf{h}}
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\W}{\mathbf{W}}
\newcommand{\vb}{\mathbf{b}}
\newcommand{\vc}{\mathbf{c}}
\newcommand{\txtcolb}[1]{\textcolor{blue}{\Large #1}}

% Insert presentation title here
\newcommand{\presentationtitle}{Deep Neural Network for Speech Emotion Recognition}
\newcommand{\shortpresentationtitle}{Deep Learning}

% Insert type of presentation here (or comment line), probably one of:
% Mitarbeitervortrag, Bachelor-Arbeit, Master-Arbeit, Bachelor thesis, Master thesis
\newcommand{\presentationtype}{---A Study of Deep Learning---}

% Insert presentation date here
\newcommand{\presentationdate}{05.06.2014}

% Uncomment the following line, if you write in English
%\newcommand{\lang}{german}

% Uncomment the following line, if you want to create handouts (setting to false does not work!)
% \newcommand{\handoutmode}{true}

% Load beamer class using LSS style
\input{presentation}

\usepackage{setspace}
\usepackage{graphicx}
\usepackage{pgfpages}
\graphicspath{ {../Fig/} }
% \setbeameroption{show notes on second screen=left}
% \setbeameroption{second mode text on second screen=left}
% \setbeameroption{show notes}
% \setbeameroption{show notes on second screen=left}


% My commands:

% -----------------------------------------------------------------------------
% -----------------------------------------------------------------------------
\begin{document}
\lstset{basicstyle=\small\ttfamily,xleftmargin=15pt,language=Matlab,
        commentstyle=\color{green},showstringspaces=false,stringstyle=\color{magenta}\ttfamily}

% -----------------------------------------------------------------------------
% This is the title page
\begin{frame}[t,plain]
	\titlepage
\end{frame}


% -----------------------------------------------------------------------------
% Motivation slide
\begin{frame}[t]{Motivation} % 1 folie
% \textcolor{blue}{\Large Training objective}
\uncover<1->{
\textcolor{blue}{\Large Why speech emotion recognition}
	\begin{itemize}
% 		\itemsep40pt
		\item Most current work focuses on speech processing based on linguistic information,  e.g.: Skype Translator
		
		\item More natural human-machine interaction requires paralinguistic information such as age, gender, emotion.
		\item Speech Recognition / Speeker Identification / Emotion Recognition
		\begin{minipage}[h]{0.48\linewidth}
		 \includegraphics[width=\linewidth]{paraliguistic.png}
		\end{minipage}

	\end{itemize}
}

\uncover<2->{
\textcolor{blue}{\Large Deep Network Applications}
	\begin{itemize}
		\item Handwriting Digit Recognition
		\item Image Recognition
	\end{itemize}
}
\end{frame}

% -----------------------------------------------------------------------------
% This is the table of contents. You can insert a motivation before or after this slide.
\begin{frame}
	\ifthenelse{\equal{\lang}{ngerman}}{
		\frametitle{Table of Contents}
	}{
		\frametitle{Table of Contents}
	}
	\tableofcontents
\end{frame}

% Add an extra slide at the beginning of each section while highlighting the current section
% Use \section* to skip the slide once or comment the following to skip all overview slides.
\AtBeginSection[]
{
	\begin{frame}<beamer>
		\ifthenelse{\equal{\lang}{ngerman}}{
			\frametitle{Table of Contents}
		}{
			\frametitle{Table of Contents}
		}
% 		\frametitle{\contentsname}
		\tableofcontents[currentsection]
	\end{frame}
}

%% =========
\section{Foundations} %2 folies
% -----------------------------------------------------------------------------
\subsection{Mel Frequency Cepstral Features}
	\begin{frame}[t]{Mel Frequency Cepstral Features}
		\begin{minipage}[t]{0.48\linewidth}
			\begin{itemize}
				\item short-term power spectrum 
				\item mel-scale approximate human perception
				\item widely-used in speech recognition tasks
				\item Transformation between Mel and Hertz scale
				\begin{figure}

				\begin{eqnarray}
				f_{mel} = 1125~\ln~(1+f_{Hz}/700)\\
				f_{Hz} = 700 \left(\exp(f_{mel}/1125)-1\right)
				\end{eqnarray}

			\end{figure}
			\end{itemize}
		\end{minipage}
		\begin{minipage}[t]{0.48\linewidth}
		\begin{figure}
		 \includegraphics[width = \linewidth]{MelvsHz.png}
		\end{figure}
		\end{minipage}
	\end{frame}


\subsection{Emotion Recognition Approaches}

\begin{frame}[t]{Emotion Recognition Approaches}
	\begin{minipage}[t]{0.48\linewidth}
	  \textcolor{blue}{\Large Traditional Approaches}
	  \begin{itemize}
	   \item pre-selected features
	   \item supervised training
	   \item low-level features not appropriate for claasification
	   \item shallow structure of classifiers
	  \end{itemize}
	\end{minipage}\hfill
	\begin{minipage}[t]{0.48\linewidth}
	\textcolor{blue}{\Large Deep Learning Approaches}
	  \begin{itemize}
	   \item learning representations from high-dim data
	   \item extracting appropriate features without hand-crafting
	   \item low-level features are used to build high-level features as network gets deeper
	   \item frame-based classfication
	  \end{itemize}

	\end{minipage}

\end{frame}


\section{Conditional Restricted Boltzmann Machine}
    \subsection{Restricted Boltzmann Machine}
	\begin{frame}[t]{Character}
% 	\txtcolb{}
	 \begin{itemize}
	  \item Generative model 
	  \item Undirected graphical model, good at modeling high-dimensioanl data (speech emotion)
	  \item Trained in unsupervised way, only use unlabeled input sequence$\x$ for learning. \note{clustering, mixture models,PoE}
		  \begin{itemize}
		   \item automatically extract useful features from data 
		   \item Find hidden structure (distribution). 
		   \item Learned features used for prediction or classification
		  \end{itemize}
% 	  \item speicifies a joint distribution over input and hidden variables, can either generating data, or with bayesian
% rule to form conditional distribution. 
	  \item Potential to be extend to capture temporal information.
	 \end{itemize}


	
% 	\begin{minipage}[t]{\linewidth}
% 	 $\x$, input units\\
% 	 $\h$, hidden units
% 	\end{minipage}

	\end{frame}
	\begin{frame}{Structure}
	    \begin{figure}[t]
	    \includegraphics[width=0.5\linewidth]{RBMStruct.png}
	    \end{figure}
	    \begin{minipage}{0.48\linewidth}
	    \begin{align}
	      \text{Energy Function:}~&E_{\boldsymbol{\theta}} = -\mathbf{x^{T}}\mathbf{W}\mathbf{h}-\mathbf{b^{T}}\mathbf{x}-\mathbf{c^{T}}\mathbf{h}\nonumber\\
	    \text{Joint Distribution:}~&P^{RBM} (\mathbf{x},\mathbf{h}) = \frac{1}{Z} e^{-E_{\boldsymbol{\theta}}(\mathbf{x},\mathbf{h})}\nonumber\\
	     \text{Partition Function:}~ &Z = \sum_{\mathbf{x,h}} e^{-E_{\boldsymbol{\theta}}(\mathbf{x},\mathbf{h})}\nonumber
	    \end{align}
	    \end{minipage}
	    
	\end{frame}

	\begin{frame}
	\txtcolb{Inference}
		\begin{minipage}[t]{0.48\linewidth}
		\begin{figure}[t]
			 \begin{align}
			 &P(h_{j}=1 \mid \mathbf{x})= sigmoid(\sum_{i} x_{i}W_{ij} + c_{j})\nonumber\\
			 &P(x_{i}=1 \mid \mathbf{h}) = sigmoid(\sum_{j} W_{ij} h_{j} + b_{i})\nonumber
			 \end{align}
		\end{figure}
	\end{minipage}
% 	\begin{minipage}{0.48\linewidth}
% 	$\x$ input units
% 	$\h$ hidden units
% 	$W$  weight connections
% 	\end{minipage}

	\vspace{5mm}
	 
	\end{frame}
	
	\begin{frame}{Training}
	 \txtcolb{Sampling}
	\end{frame}



  
%% =========
\section{Deep Neural Networks}%4 folies
	\subsection{Concept}		
		\begin{minipage}[t]{0.48\linewidth}
		\begin{figure}
			Computing net-activation
			\begin{eqnarray}
				\underline{z}^{(l+1)}_k &=& {\bf W}^{(l)}\underline{a}^{(l)}_k + \underline{b}^{(l)} \nonumber
\\
				\underline{a}^{(l+1)}_k &=& \underline{\Phi} \left ( \underline{z}^{(l+1)}_k \right )
\nonumber\\
				\hat{\underline{y}}_k &=& \underline{a}^{(ol)}_k \nonumber
			\end{eqnarray}
		\end{figure}
		\end{minipage}\hfill
		\begin{minipage}[t]{0.48\linewidth}
			\begin{itemize}
				\item Arbitrary non-linear mapping from $\underline{x}_k$ to  $\hat{\underline{y}}_k$ possible
				\item Relation $N \Leftrightarrow$ Complexity
				\item Deep Architectures $(l \uparrow)$ more efficient than shallow ones ($l\downarrow,
N_l\uparrow$)
			\end{itemize}
		\end{minipage}
		
	
	\begin{frame}[t]{Determining the parameters}
		\textcolor{blue}{\Large Training objective}
			\begin{eqnarray}
				J({\bf W}, \underline{b}) &=& \sum_{\forall k} \frac{1}{2} ||\underline{y}_k -
\hat{\underline{y}}_k||^2 + \frac{\lambda}{2} \sum_{\forall l} ||{\bf W}^{(l)}||_F^2 \\
				{\bf W}, \underline{b} &=& \arg \min_{{\bf W}, \underline{b}} J({\bf W}, \underline{b})
			\end{eqnarray}
			
		\textcolor{blue}{\Large Numerical minimization}		
		\begin{itemize}
			\item Gradient calculation with Backpropagation
			\item Stochastic gradient descent
			\item {\bf L}imited memory {\bf B}royden-{\bf F}letcher-{\bf G}oldfarb-{\bf S}hanno (L-BFGS)
		\end{itemize}	
	\end{frame}
	
	
	\subsection{Problems and Solutions}
	\begin{frame}[t]{Problems}
		\begin{itemize}
			\item Optimization problem non-convex\\
			$\Rightarrow$ getting stuck in poor local minima
			\item Diffusion of gradients
			\item Large p small n problem $\Rightarrow$ overfitting
	
	\end{itemize}

	\end{frame}
	\begin{frame}[t]{Solutions}
		\begin{minipage}[h]{0.48\linewidth}
			\begin{itemize}
				\item Layerwise Pre-training
			\end{itemize}
		\end{minipage}\hfill
		\begin{minipage}[h]{0.48\linewidth}
			\begin{itemize}
				\item Layerwise Pre-training
			\end{itemize}
		\end{minipage}		
	\end{frame}

\section{Long Short Term Memory}
	\subsection{Recurrent Neural Network}
		\begin{frame}[t]{Recurrent Neural Network}
% 			\begin{minipage}[t][][t]{0.48\linewidth}
			\textcolor{blue}{\Large Concepts of RNN}
			\pause
				\begin{itemize}[<+->]
% 				 \itemsep10pt
				 \item modelling sequential data, emotion in speech .
				 \item Same Structure as MLP but differs from feed-forward network, enabling
nonlinear mapping.		\note<2>{tell me a story.}
				 \only<3>{\begin{figure}
					\begin{eqnarray}
						&h_{t} = \mathcal{H}(W_{xh}x_{t}+W_{hh}h_{t-1} + b_{h})\nonumber\\
						&y_{t} = W_{hy}h_{t} + b_{y}\nonumber
					\end{eqnarray}

				 \end{figure}
				 }
				
				 \item Feedback connection between previous hidden units and current hidden units, enabling
memory past hidden state.
				 \item Potentially to model arbitary dynamic system.
				 \item Trained with \textbf{b}ack\textbf{p}ropagation \textbf{t}hrough \textbf{t}ime (BPTT)
				 \note{natural extension of BP in FF network}
				\end{itemize}
% 			\end{minipage}
\vspace{5mm}
% 		\only<1>{\begin{figure}
% 		          \includegraphics[width=0.5\textwidth]{NeuralNetwork.png}
% 		         \end{figure}
% 
% 		}
		\end{frame}
		
		\begin{frame}[t]{Recurrent Neural Network}
% 			\begin{minipage}[t][][t]{0.48\linewidth}
			\textcolor{blue}{\Large Concepts of RNN}
			\begin{minipage}[t]{0.48\linewidth}
				\only<1>{
				\begin{figure}[t]
				\includegraphics[width=0.8\textwidth]{RNNunit.png}
				\end{figure}
				}
			\end{minipage}\hfill
			\begin{minipage}[t]{0.48\linewidth}
				\only<1>{
				\begin{figure}[t]
				\includegraphics[width=\textwidth]{RNNStruct.png}
				\end{figure}
				}
			\end{minipage}

		\end{frame}
		
		\begin{frame}[t]{From RNN to LSTM}
		\textcolor{blue}{\Large Problems with RNN}
			\begin{itemize}%[<+->]
			 \item gradient vanishing during backpropagation as time steps increases (>100)
			 \item difficult to capture long-time dependency (which is required in emotion recognition)
			\end{itemize}
		\textcolor{red}{\Large Solutions}	
			\begin{itemize}
			 \item 
			\end{itemize}
    
    
		\end{frame}
		\begin{frame}{Long short term memory}
		\only<1>{S. Hochreiter and J. Schmidhuber, Lovol. 9, pp. 1735-1780, 1997.}
		\uncover<2->{
		\textcolor{blue}{\Large LSTM unit}\\
		\begin{minipage}{0.4\linewidth}
			\begin{figure}
			 \includegraphics[width=\linewidth]{LSTMstruct.png}
			\end{figure}
		\end{minipage}
		\begin{minipage}[t]{0.55\linewidth}
			\begin{figure}
			 \begin{eqnarray}
				  i_{t} &=& \sigma (W_{xi}x_{t} + W_{hi}h_{t-1} + W_{ci}c_{t-1} +b_{i})\nonumber\\
				  f_{t} &=& \sigma (W_{xf}x_{t} + W_{hf}h_{t-1} + W_{cf}c_{t-1} +b_{f})\nonumber\\
				  c_{t} &=& f_{t}c_{t-1} + i_{t}\mathrm{tanh}(W_{xc}x_{t} + W_{hc}h_{t-1} + b_{c})\nonumber\\
				  o_{t} &=& \sigma (W_{xo}x_{t} + W_{ho}h_{t-1} + W_{co}c_{t} +b_{o})\nonumber\\
				  h_{t} &=& o_{t}\mathrm{tanh} (c_{t})\nonumber
			 \end{eqnarray}
			\end{figure}
		\end{minipage}

		}
		\end{frame}
		
		\begin{frame}{Long short term memory}
		\textcolor{blue}{\Large Features in LSTM}
		 \begin{itemize}
		  \item gates are trained to learn when it shoud be open/closed. 
		  \item Constant Error Carousel
		  \item preserve long-time dependency by maintaining gradient over time. 
		 \end{itemize}
		 \begin{figure}
		  \includegraphics[width=\textwidth]{LSTMGra.png}
		 \end{figure}


		\end{frame}

\section{Experiments}

\section{Conclusion and Outlook}


\end{document}
